# [论文阅读]Motif Prediction with Graph Neural Networks, KDD, August 14-18, 2022

## 总体介绍

本篇发表在KDD'22, August 14-18, 2022 ，作者是来自苏黎世联邦理工学院(ETH Zurich)的Maciej Besta等人。

©2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9385-0/22/08. . . $15.00
https://doi.org/10.1145/3534678.3539343

链路预测问题是图挖掘的核心问题之一。最近研究强调了高阶网络分析(higher-order network analysis)的重要性，研究motif的复杂结构成为很重要的工作。现有的链路预测方案不能够很好的解决motif的预测问题，同时也为了提高准确性，所以本篇工作开发了一个基于GNN的motif预测框架。

本文的贡献如下：

(1) 识别并且公式化了对于motif预测问题的评分函数。

(2) 分别展示了如何用启发式的方法和GNN来解决motif的预测问题。

(3) 说明了GNN是比启发式方法更有效的。

## 准备工作

关于链接预测和图神经网络的介绍就不再赘述。这里主要介绍文中讲解的Motif预测的相关概念。

### motif预测问题的建立

本篇工作定义待预测的motif为M = （Vm，Em），Vm是motif点集，Em是motif的边集，不管这条边在不在数据集里，可能已经存在，也可能不存在。

### motif预测和链接预测的异同

（M）There May Be **Many** Potential New Motifs For a Fixed Vertex Set

就是说，链接预测是个二元问题，找的是不相连两点之间的一条链接。而motif预测的问题更复杂，因为有|Vm|个点，每两个点都有可能产生边，那么最多就有组合数C = C(|Vm|,2)条边，对于要预测的motif的选择，每条边要么包含要么不包含，所以是2的C次方，再排除空集，所以共有2^[C(|Vm|,2)] - 1个实例。

（E）There May Be **Existing** Edges

意思是链接预测肯定预测的是不相邻两点之间，但motif预测的图形是可以已经存在边的。

（D）There May Be “**Deal-Breaker**” Edges

意思是motif预测问题中，存在一些边可能是不太可能或者说不可能出现的，叫做”交易破坏者“的这种边，这种边的存在影响到motif预测问题。

（L）Motif Prediction Query May Depend on Vertex **Labeling**

motif预测依赖于对于点的标签。你比如预测一个星形motif，肯定会把标签1给到星形motif的中心节点。

## 主要框架SEAM



![image-20221125193528755](https://raw.githubusercontent.com/bone38ljtnn/picture/main/1.png)

### 总体介绍

接下来介绍一下本篇工作的核心的架构，就是被称为SEAM的motif预测框架，框架流程如上图所示。

大致的流程就是，首先，确定要提取的motif的实例，然后按照在不在motif实例给边划分到Em集和-Em集，找到正样本Gp和负样本Gn，在样本基础上找到h跳邻居节点；然后是做一个节点嵌入，把嵌入向量加到特征矩阵，然后给带有h跳邻居节点的样本赋予内标签和外标签，创建邻接矩阵，把样本子图的特征矩阵输入到DGCNN模型进行训练和预测。

接下来详细介绍每一部分。

### 指定要预测的motifs

本篇工作提出的SEAM框架给用户提供了选择。

用户可以选择待预测的motif的点集Vm和边集Em，以及潜在的deal-breaker的边集-Em,d。可以知道选定Vm点集后，最多可以选择2^[C(|Vm|,2)] - 1个motif实例。重复解释一下，因为有|Vm|个点，每两个点都有可能产生边，那么最多就有组合数C = C(|Vm|,2)条边，对于要预测的motif的选择，每条边要么包含要么不包含，所以是2的C次方，再排除空集，所以共有2^[C(|Vm|,2)] - 1个实例。

每一条边都属于不同的集合。文中的介绍如图。

![image-20221127155241138](https://raw.githubusercontent.com/bone38ljtnn/picture/main/2.png)

其中蓝色的线代表包含在要预测motif实例的边，红色线代表不包含在motif的边，黑色则是无关紧要的边。其中，实线表示数据集已经存在的边，虚线表示不存在的边。例如蓝色虚线Em,e表示的是包含在motif的边且在数据集中不存在，红色实线-Em,d,e则表示不包含在motif的边且在数据集中已经存在。

### 找正样本和负样本

#### 正样本

对于正样本的寻找，非常简单，因为我们要预测的motif实例都是用户自己指定的，这些实例就作为正样本呢。

#### 负样本

而负样本的寻找就比较有挑战性。本篇工作使用三种策略来寻找负样本。

（1）从正样本中移掉少量的点，再从附近找几个点添加上，这样的话仅仅在正样本的基础上减了少量的边或者添了少量的deal-breaker边,总的来说此方法找到的负样本跟正样本是十分相似的。

（2）在Vm点集里随机采样顶点，这样的负样本一般连接比较稀疏，并且跟正样本不相似。

（3）先随即找一个点r，加到空集里，然后一直添加该集合邻域的点到集合里，直到size跟Vm一样。此方法找到的负样本呢一定程度上跟正样本相似。

本篇工作最终的负样本集采用的是80%的（1）方法和各10%的（2）、（3）方法构成的样本。

采用各10%的（2）、（3）方法也是用来避免模型训练时候出现过拟合的问题。

选择等量的正样本和负样本，保持数据集的平衡。然后训练集和测试集的分割通常是9/1，最后为了确保正负样本在数据集中均匀分布，做了样本的随机排列。

### 提取样本子图

本篇工作为了减少成本，但又避免样本边缘点信息的丢失，就采用了提取近距离环境的子图样本，也就是只提取1-2跳（hops）的邻居节点。使用了简单的遍历来寻找h跳的点集。

### 构造节点的嵌入

仍然是边缘信息丢失的问题，提取的带有h跳邻居样本子图，在某些情况下也会丢失一些带预测的motif的信息。为了缓解这一问题，本篇工作又做了一个节点嵌入Xe，采用随机游走(random walks)的方式对更远的图的区域信息进行编码。本篇采用的是node2vec方法以及实用的来自DeepWalk的参数。其中Xe∈R^(n*f)，f是节点的低维向量表示的维数。

把嵌入向量加到样本子图的特征矩阵里。

### 按结构特点给节点加标签



在样本子图中，其中属于正样本和负样本的节点Vp,Vn，叫做内（inner）节点，其余的h跳邻居节点称为外（outer）节点。

内标签就是内节点的枚举。它一般根据每个内节点扮演的角色来进行排序。就是比如说要预测一个k-star，那中心节点理所应当就是内标签的1号。

然后内标签转换成一个独热（one-hot）矩阵H∈N^(k*k)。其中Hij代表的是，第i个节点的内标签是j。

为了把H加到样本子图的特征矩阵里，把H连接一个零矩阵0[(s-k)k]得到Xh = （H 0）T，一个s*k的矩阵。



外标签则是外节点的枚举。外标签转换为一个邻接矩阵，L∈N^(s-k)*k，存储的是每一个外节点都指定到每个内节点之间的距离，即Lij表示外标签为i的节点到内标签为j节点的距离。也链接一个零矩阵0kk，得到sk的矩阵。

内标签邻接矩阵Xh、外标签邻接矩阵Xl，再加上嵌入矩阵Xe以及特征矩阵Xsi，构成输入矩阵Xs

### 使用GNN的模型训练

使用的模型是DGCNN(AAAI2018)，是SEAL模型中使用的方法。

该模型的第一级由三个图卷积层组成，用来把节点的特征信息散发到邻居节点。然后把每个图卷积层的输出都输入到一个名叫k-sortpooling的排序层进行排序，接下来是一层标准的1D卷积层，然后是致密层，softmax层，以获得预测概率。通过这些输入，训练100次。最终的期望是能够把正样本预测为motif而负样本不被预测为motif。



## 实验

![image-20221128161525783](https://raw.githubusercontent.com/bone38ljtnn/picture/main/3.png)

第一个表5的实验，baseline是Common Neighbors，Adamic-Adar，Jaccard三个链接预测的方法以及使用GNN的SEAL方法，然后使用的链接预测分数是Mul，Min，Avg三种，评估标准是AUC，即曲线下面积，一种评估分类模型准确性的度量标准。数据集是USAir和Yeast。

大概能从实验知道四点：

（1）首先是SEAM方法在结果上相比于其他baseline具有很强的优越性。SEAM也随着待预测的motif的边越来越大或者说规模越来越大的时候，预测的精确度更高。可见SEAM是捕获了不同边之间的相关性的。这种优越性对于dbstar竟然也是适用。

（2）SEAM去掉嵌入方法，结果上与SEAM相比准确度有所下降。我的理解是，带有嵌入模块的SEAM通过随机游走的方式给数据集提供了样本子图中丢失的部分边缘数据，捕获了更多的边缘点之间的相关性，所以精确度更高。

（3）我们发现在三种baseline中，使用Avg和Min分数也一定程度上改进了使用Mul分数的准确性。虽然我们核心工作是SEAM，但是仍然不能够否定启发式方法的特点，比如不需要训练，总体上时间更短等。

（4）CN方法表现非常差。原因是这种方法必须预测边的相似邻域，而本篇工作选取负样本时，选取的都是跟正样本十分相似的，所以导致了这种方法准确性是最低的。



![image-20221128162821218](https://raw.githubusercontent.com/bone38ljtnn/picture/main/4.png)

表6表7两个实验结果本别是对Yeast数据集预测密集motifs的结果，对Power稀疏数据集预测结果。两个实验具有与表5实验相似的实验结果。
  
  
 # [RETURN](bone38ljtnn.github.io)   ,   [TOP](#)










